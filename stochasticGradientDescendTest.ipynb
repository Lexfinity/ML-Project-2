{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stochasticGradientDescendTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVPkfSro5RZV",
        "colab_type": "code",
        "outputId": "d5b196b8-5183-4a3e-df79-fa82dc135348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "redditData = pd.read_csv(\"/content/reddit_train.csv\") #, sep=\"\\n\", header=None) \n",
        "\n",
        "commentsTrain = redditData.iloc[:,1]\n",
        "subredditsTrain = redditData.iloc[:,-1]\n",
        "\n",
        "GX_train = commentsTrain[commentsTrain.index < np.percentile(commentsTrain.index, 80)].sort_index()\n",
        "Gy_train = subredditsTrain[subredditsTrain.index < np.percentile(subredditsTrain.index, 80)].sort_index()\n",
        "\n",
        "GX_train = [str (item) for item in GX_train]\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "GX_train = tfidf.fit_transform(GX_train)\n",
        "\n",
        "# estimator = MultinomialNB()\n",
        "# param_grid = { \n",
        "#         \"alpha\": [0.1,0.15,0.2536,0.121253,0.35, 0.31, 0.3247,0.1235, 0.0879,0.3,0.2,0.01,0.8],\n",
        "#         \"fit_prior\": [True, False],\n",
        "#         }\n",
        "\n",
        "# grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
        "\n",
        "# grid.fit(GX_train, Gy_train)\n",
        "\n",
        "# print(grid.best_score_)\n",
        "# print(grid.best_params_)\n",
        "\n",
        "estimator = SGDClassifier()\n",
        "param_grid = { \n",
        "        \"loss\": [\"squared_hinge\",\"hinge\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"],\n",
        "        \"penalty\": [\"none\", \"l2\", \"l1\"],\n",
        "        \"alpha\": [0.1,0.15,0.3,0.2,0.01,0.8,0.5],\n",
        "        \"epsilon\": [0.1,0.15,0.8,0.5,0.01],\n",
        "        \"max_iter\": [10, 5, 8, 3],\n",
        "        }\n",
        "\n",
        "grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
        "\n",
        "grid.fit(GX_train, Gy_train)\n",
        "\n",
        "print(grid.best_score_)\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4365103077382731\n",
            "{'alpha': 0.01, 'epsilon': 0.01, 'loss': 'epsilon_insensitive', 'max_iter': 5, 'penalty': 'l2'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9LsQSV77Vnt",
        "colab_type": "code",
        "outputId": "d452c1fd-8be8-4134-f991-b7edf82a0e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "redditData = pd.read_csv(\"/content/reddit_train.csv\") #, sep=\"\\n\", header=None) \n",
        "\n",
        "commentsTrain = redditData.iloc[:,1]\n",
        "subredditsTrain = redditData.iloc[:,-1]\n",
        "\n",
        "GX_train = commentsTrain[commentsTrain.index < np.percentile(commentsTrain.index, 80)].sort_index()\n",
        "Gy_train = subredditsTrain[subredditsTrain.index < np.percentile(subredditsTrain.index, 80)].sort_index()\n",
        "\n",
        "GX_train = [str (item) for item in GX_train]\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "GX_train = tfidf.fit_transform(GX_train)\n",
        "sgd = SGDClassifier(alpha=0.01, epsilon=0.01, loss='epsilon_insensitive', max_iter=5, penalty= 'l2')\n",
        "print(\"ready\")\n",
        "scores = cross_val_score(sgd, GX_train, Gy_train, cv=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ready\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yz21_ufeQ2p",
        "colab_type": "code",
        "outputId": "703cd2ab-019c-4824-e2b7-be173fc6b567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.52909128, 0.53748843, 0.53483585, 0.53355981, 0.53099259])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbvwFVHspyYC",
        "colab_type": "code",
        "outputId": "8f86b2b3-e0f0-4fcb-a098-d2edd81d17ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.mean(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.533193592830614"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dltKwYvc08UE",
        "colab_type": "code",
        "outputId": "bcb576d6-94f5-4331-88df-4c76d51d427c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.std(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0029308748577118625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPo2-38e0_zu",
        "colab_type": "code",
        "outputId": "7627d504-6463-48ef-86a0-13db1d50a543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn import svm\n",
        "estimator = svm.SVC()\n",
        "param_grid = { \n",
        "        \"C\": [1,3,5,10],\n",
        "        \"degree\": [1,3,5,8],\n",
        "        \"gamma\": ['scale',1,3,5,10],\n",
        "        \"verbose\": [True, False],\n",
        "        \"max_iter\": [10, 5, 8, 3],\n",
        "        }\n",
        "\n",
        "grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
        "\n",
        "grid.fit(GX_train, Gy_train)\n",
        "\n",
        "print(grid.best_score_)\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]0.09534098859941556\n",
            "{'C': 1, 'degree': 1, 'gamma': 1, 'max_iter': 10, 'verbose': True}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDJkyAmQDiz7",
        "colab_type": "code",
        "outputId": "e6a94eb7-080a-4523-df94-b8a5039db96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "grid.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c404952e5198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RNGLYJ0H3nc",
        "colab_type": "code",
        "outputId": "3d5dd2a0-09e8-4cc4-9b06-56cd62d72c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "estimator = LogisticRegression()\n",
        "param_grid = { \n",
        "        \"C\": [1,3,5,10],\n",
        "        \"penalty\": ['l1', 'l2'],\n",
        "        \"max_iter\": [10, 5, 8, 3],\n",
        "        }\n",
        "\n",
        "grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
        "\n",
        "grid.fit(GX_train, Gy_train)\n",
        "\n",
        "print(grid.best_score_)\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5335638144626909\n",
            "{'C': 3, 'max_iter': 10, 'penalty': 'l2'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM6SZm5N0quT",
        "colab_type": "code",
        "outputId": "79795fde-210a-4acf-f980-b01c0874df5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numbers\n",
        "import decimal\n",
        "import scipy.stats as ss\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import stdev\n",
        "from statistics import mean\n",
        "import time\n",
        "import math\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wW-Nv6lmI_Z",
        "colab_type": "code",
        "outputId": "3ca5e190-e636-4b8c-b02f-9323420c79b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import string\n",
        "\n",
        "redditDataTrain = pd.read_csv(\"/content/reddit_train.csv\") #, sep=\"\\n\", header=None) \n",
        "commentsTrain = redditDataTrain.iloc[:,1]\n",
        "subredditsTrain = redditDataTrain.iloc[:,-1]\n",
        "\n",
        "def remove(text):\n",
        "  return \"\".join([c for c in text if c not in string.punctuation])\n",
        "\n",
        "commentsTrain = commentsTrain.apply(lambda x: remove(x))\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "commentsTrain = commentsTrain.apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "\n",
        "for i in commentsTrain:\n",
        "  print(i)\n",
        "  break\n",
        "  \n",
        "def stop(text):\n",
        "  return [w for w in text if w not in stopwords.words('english')]\n",
        "\n",
        "commentsTrain = commentsTrain.apply(lambda x: stop(x))\n",
        "\n",
        "for i in commentsTrain:\n",
        "  print(i)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['honestly', 'buffalo', 'is', 'the', 'correct', 'answer', 'i', 'remember', 'people', 'somewhat', 'joking', 'that', 'buffalos', 'mantra', 'for', 'starting', 'goalies', 'was', 'win', 'a', 'game', 'get', 'traded', 'i', 'think', 'edmontons', 'front', 'office', 'was', 'a', 'travesty', 'for', 'the', 'better', 'part', 'of', '10', 'years', 'but', 'buffalos', 'systematic', 'destruction', 'of', 'the', 'term', 'competitive', 'was', 'much', 'more', 'responsible', 'for', 'the', 'change', 'to', 'the', 'draft', 'lottery']\n",
            "['honestly', 'buffalo', 'correct', 'answer', 'remember', 'people', 'somewhat', 'joking', 'buffalos', 'mantra', 'starting', 'goalies', 'win', 'game', 'get', 'traded', 'think', 'edmontons', 'front', 'office', 'travesty', 'better', 'part', '10', 'years', 'buffalos', 'systematic', 'destruction', 'term', 'competitive', 'much', 'responsible', 'change', 'draft', 'lottery']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUVUn-1fmK0u",
        "colab_type": "code",
        "outputId": "68d976f8-0b56-4fef-bb13-6a55290df061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lem(text):\n",
        "  return [lemmatizer.lemmatize(i) for i in text]\n",
        "\n",
        "commentsTrain = commentsTrain.apply(lambda x: lem(x))\n",
        "\n",
        "for i in commentsTrain:\n",
        "  print(i)\n",
        "  break\n",
        "\n",
        "stem = PorterStemmer()\n",
        "\n",
        "def stemmer(text):\n",
        "  return \" \".join([stem.stem(i) for i in text])\n",
        "\n",
        "commentsTrain = commentsTrain.apply(lambda x: stemmer(x))\n",
        "\n",
        "for i in commentsTrain:\n",
        "  print(i)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['honestly', 'buffalo', 'correct', 'answer', 'remember', 'people', 'somewhat', 'joking', 'buffalo', 'mantra', 'starting', 'goalie', 'win', 'game', 'get', 'traded', 'think', 'edmonton', 'front', 'office', 'travesty', 'better', 'part', '10', 'year', 'buffalo', 'systematic', 'destruction', 'term', 'competitive', 'much', 'responsible', 'change', 'draft', 'lottery']\n",
            "honestli buffalo correct answer rememb peopl somewhat joke buffalo mantra start goali win game get trade think edmonton front offic travesti better part 10 year buffalo systemat destruct term competit much respons chang draft lotteri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDKqoDp5mm1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GX_train = commentsTrain[commentsTrain.index < np.percentile(commentsTrain.index, 80)].sort_index()\n",
        "Gy_train = subredditsTrain[subredditsTrain.index < np.percentile(subredditsTrain.index, 80)].sort_index().values\n",
        "GX_test = commentsTrain[commentsTrain.index > np.percentile(commentsTrain.index, 80)].sort_index()\n",
        "Gy_test = subredditsTrain[subredditsTrain.index > np.percentile(subredditsTrain.index, 80)].sort_index().values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUW3uf4Amn31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "GX_train_idf = tfidf.fit_transform(GX_train)\n",
        "GX_test_idf = tfidf.transform(GX_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHVdLr5Yrbhn",
        "colab_type": "code",
        "outputId": "c67849d7-c87d-4a38-a3b0-4a8e5d0bfde5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "estimator = MultinomialNB(alpha=0.3)\n",
        "estimator.fit(GX_train_idf,Gy_train)\n",
        "print(estimator.score(GX_test_idf, Gy_test))\n",
        "\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# clf = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0)\n",
        "# clf.fit(GX_train_idf, Gy_train) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5549427936455675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWZUpj6SuqV-",
        "colab_type": "code",
        "outputId": "77dab606-dedb-4612-a584-d142e8f860b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(GX_train_idf, Gy_train) \n",
        "print(clf.score(GX_test_idf, Gy_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5141163881800971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiwGFCfs4vB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = pd.get_dummies(Gy_train).values\n",
        "y_test = pd.get_dummies(Gy_test).values\n",
        "X_train = [str(i) for i in GX_train]\n",
        "X_test = [str(i) for i in GX_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr_CjDoB9Yov",
        "colab_type": "code",
        "outputId": "6ba78ee5-b896-493b-983e-e04c02bbeaaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, SpatialDropout1D, LSTM, GlobalMaxPool1D\n",
        "\n",
        "MAX_NB_WORDS = 50000\n",
        "MAX_SEQUENCE_LENGTH = 300\n",
        "EMBEDDING_DIM = 125\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(EMBEDDING_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(20, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
        "\n",
        "accr = model.evaluate(X_train, y_train)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "accuracy = model.evaluate(X_test, y_test)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL6Zu0mi7DYI",
        "colab_type": "code",
        "outputId": "82ec0f71-ba97-40a0-91b3-c035cac7412d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.808071784564427, 0.518]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3KHeh0WlaHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}