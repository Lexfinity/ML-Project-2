{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/faizank/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numbers\n",
    "import decimal\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import stdev\n",
    "from statistics import mean\n",
    "import time\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoretWithModel(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    return model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Honestly, Buffalo is the correct answer. I rem...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ah yes way could have been :( remember when he...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>He wouldn't have been a bad signing if we woul...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Easy. You use the piss and dry technique. Let ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69995</td>\n",
       "      <td>69995</td>\n",
       "      <td>Thank you, you confirm Spain does have nice pe...</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69996</td>\n",
       "      <td>69996</td>\n",
       "      <td>Imagine how many he would have killed with a r...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69997</td>\n",
       "      <td>69997</td>\n",
       "      <td>Yes. Only. As in the guy I was replying to was...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69998</td>\n",
       "      <td>69998</td>\n",
       "      <td>Looking for something light-hearted or has a v...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69999</td>\n",
       "      <td>69999</td>\n",
       "      <td>I love how I never cry about casters because I...</td>\n",
       "      <td>GlobalOffensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           comments  \\\n",
       "0          0  Honestly, Buffalo is the correct answer. I rem...   \n",
       "1          1  Ah yes way could have been :( remember when he...   \n",
       "2          2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...   \n",
       "3          3  He wouldn't have been a bad signing if we woul...   \n",
       "4          4  Easy. You use the piss and dry technique. Let ...   \n",
       "...      ...                                                ...   \n",
       "69995  69995  Thank you, you confirm Spain does have nice pe...   \n",
       "69996  69996  Imagine how many he would have killed with a r...   \n",
       "69997  69997  Yes. Only. As in the guy I was replying to was...   \n",
       "69998  69998  Looking for something light-hearted or has a v...   \n",
       "69999  69999  I love how I never cry about casters because I...   \n",
       "\n",
       "            subreddits  \n",
       "0               hockey  \n",
       "1                  nba  \n",
       "2      leagueoflegends  \n",
       "3               soccer  \n",
       "4                funny  \n",
       "...                ...  \n",
       "69995           europe  \n",
       "69996  leagueoflegends  \n",
       "69997           canada  \n",
       "69998            anime  \n",
       "69999  GlobalOffensive  \n",
       "\n",
       "[70000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditDataTrain = pd.read_csv(\"data/reddit_train.csv\") #, sep=\"\\n\", header=None) \n",
    "redditDataTest = pd.read_csv(\"data/reddit_test.csv\") # sep=\"\\n\", header=None)\n",
    "redditDataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentsTrain = redditDataTrain.iloc[:,1]\n",
    "subredditsTrain = redditDataTrain.iloc[:,-1]\n",
    "commentsTest = redditDataTest.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.naive_bayes import MultiNomialNB as CustomNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.01\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "cv = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "multiNB = MultinomialNB()\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "cnb = CustomNaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=4)\n",
    "kf = KFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GX_train, GX_test, Gy_train, Gy_test = train_test_split(commentsTrain, subredditsTrain, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GX_train = commentsTrain[commentsTrain.index < np.percentile(commentsTrain.index, 80)].sort_index()\n",
    "Gy_train = subredditsTrain[subredditsTrain.index < np.percentile(subredditsTrain.index, 80)].sort_index()\n",
    "GX_test = commentsTrain[commentsTrain.index > np.percentile(commentsTrain.index, 80)].sort_index()\n",
    "Gy_test = subredditsTrain[subredditsTrain.index > np.percentile(subredditsTrain.index, 80)].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so I am splitting the global train data into Gtest and Gtrain. Then I use k-fold on Gtrain. in the k-fold I select the model that gives the best accuracy. Then I use that model, to predict the global thing. Ideally I should be using k-fold for validating which theory about the data is correct. Then train the validation theory, using the entire Gtrain and predict Gtest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get avg accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_acc_for_model(model, **kwargs):\n",
    "    results = []\n",
    "    for train_index, test_index in kf.split(GX_train, Gy_train):\n",
    "        x_train, x_test, y_train, y_test = GX_train[train_index], GX_train[test_index], Gy_train[train_index], Gy_train[test_index]\n",
    "        redditDataTrainTF = tfidf.fit_transform(x_train)\n",
    "        redditDataTestTF = tfidf.transform(x_test)\n",
    "        redditDataTrainTF.toarray()\n",
    "        clf = model(**kwargs)\n",
    "        results.append(getScoretWithModel(clf, redditDataTrainTF, redditDataTestTF, y_train, y_test))\n",
    "    avg_acc = sum(results)/len(results)\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression, MultinomialNB, CustomNaiveBayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test for model <class 'sklearn.linear_model.logistic.LogisticRegression'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5355714285714286\n",
      "test for model <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "0.5502857142857143\n",
      "test for model <class 'models.naive_bayes.MultiNomialNB'>\n",
      "smoothing factor 0.01\n",
      "smoothing factor 0.01\n",
      "smoothing factor 0.01\n",
      "smoothing factor 0.01\n",
      "0.5352321428571429\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(\"test for model\", model)\n",
    "    print(get_avg_acc_for_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look for the best smoothing factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1501,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.001\n",
      "smoothing factor 0.001\n",
      "smoothing factor 0.001\n",
      "smoothing factor 0.001\n",
      "smoothing factor 0.001\n",
      "0.519267181569372\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.0001\n",
      "smoothing factor 0.0001\n",
      "smoothing factor 0.0001\n",
      "smoothing factor 0.0001\n",
      "0.5066964285714286\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 1e-05\n",
      "smoothing factor 1e-05\n",
      "smoothing factor 1e-05\n",
      "smoothing factor 1e-05\n",
      "0.49926785714285715\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.01\n",
      "smoothing factor 0.01\n",
      "smoothing factor 0.01\n",
      "smoothing factor 0.01\n",
      "0.5352321428571429\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.02\n",
      "smoothing factor 0.02\n",
      "smoothing factor 0.02\n",
      "smoothing factor 0.02\n",
      "0.5413928571428571\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.03\n",
      "smoothing factor 0.03\n",
      "smoothing factor 0.03\n",
      "smoothing factor 0.03\n",
      "0.5443214285714285\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5529821428571429\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 1\n",
      "smoothing factor 1\n",
      "smoothing factor 1\n",
      "smoothing factor 1\n",
      "0.4699285714285714\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard tfidf accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5529821428571429\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5532678571428571\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', sublinear_tf=True)\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5534285714285714\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', sublinear_tf=True, strip_accents='ascii')\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5534285714285714\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', sublinear_tf=True, strip_accents='ascii', ngram_range=(1, 1))\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5534285714285714\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', sublinear_tf=True, strip_accents='ascii', ngram_range=(1, 1), lowercase=True)\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5420178571428571\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, stop_words='english', sublinear_tf=True, strip_accents='ascii', ngram_range=(1, 1), lowercase=True)\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test tokenizer, best so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5556607142857143\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words='english', sublinear_tf=True, strip_accents='ascii', ngram_range=(1, 1), lowercase=True, tokenizer = token.tokenize)\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "smoothing factor 0.1\n",
      "0.5536964285714285\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, stop_words=stop_words, strip_accents='ascii', ngram_range=(1, 1), lowercase=True)\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stem_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ad11fbe29b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstem_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_avg_acc_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomNaiveBayes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stem_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', sublinear_tf=True, strip_accents='ascii', ngram_range=(1, 1), lowercase=True, tokenizer = stem_tokenize)\n",
    "print(get_avg_acc_for_model(CustomNaiveBayes, smoothing_factor=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gx_train, Gx_test, y_train, y_test = train_test_split(commentsTrain, subredditsTrain, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\", smooth_idf=True, sublinear_tf=True, use_idf=True, strip_accents='unicode',min_df=1, tokenizer = stem_tokenize) #, max_df=0.95 )\n",
    "GX_train_idf_tokenized = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf_tokenized = tfidf.transform(Gx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrain with the entire model and test final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/faizank/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 'NN'),\n",
       " ('h', 'NN'),\n",
       " ('i', 'JJ'),\n",
       " ('s', 'VBP'),\n",
       " (' ', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('s', 'VBP'),\n",
       " (' ', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " (' ', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('s', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('!', '.'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=\"this is a test!.\"\n",
    "sent_tokens = word_tokenize(sent)\n",
    "nltk.pos_tag(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gx_train, Gx_test, y_train, y_test = train_test_split(commentsTrain, subredditsTrain, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_tokenizer(stop_words):\n",
    "    tokens = word_tokenize(stop_words)\n",
    "    return nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'IN'),\n",
       " ('me', 'PRP'),\n",
       " ('my', 'PRP$'),\n",
       " ('myself', 'PRP'),\n",
       " ('we', 'PRP'),\n",
       " ('our', 'PRP$'),\n",
       " ('ours', 'PRP'),\n",
       " ('ourselves', 'PRP'),\n",
       " ('you', 'PRP'),\n",
       " (\"you're\", 'VBP'),\n",
       " (\"you've\", 'JJ'),\n",
       " (\"you'll\", 'NN'),\n",
       " (\"you'd\", 'VB'),\n",
       " ('your', 'PRP$'),\n",
       " ('yours', 'NNS'),\n",
       " ('yourself', 'PRP'),\n",
       " ('yourselves', 'VBZ'),\n",
       " ('he', 'PRP'),\n",
       " ('him', 'PRP'),\n",
       " ('his', 'PRP$'),\n",
       " ('himself', 'PRP'),\n",
       " ('she', 'PRP'),\n",
       " (\"she's\", 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('hers', 'NNS'),\n",
       " ('herself', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " (\"it's\", 'VBZ'),\n",
       " ('its', 'PRP$'),\n",
       " ('itself', 'PRP'),\n",
       " ('they', 'PRP'),\n",
       " ('them', 'PRP'),\n",
       " ('their', 'PRP$'),\n",
       " ('theirs', 'JJ'),\n",
       " ('themselves', 'PRP'),\n",
       " ('what', 'WP'),\n",
       " ('which', 'WDT'),\n",
       " ('who', 'WP'),\n",
       " ('whom', 'WP'),\n",
       " ('this', 'DT'),\n",
       " ('that', 'WDT'),\n",
       " (\"that'll\", 'VBZ'),\n",
       " ('these', 'DT'),\n",
       " ('those', 'DT'),\n",
       " ('am', 'VBP'),\n",
       " ('is', 'VBZ'),\n",
       " ('are', 'VBP'),\n",
       " ('was', 'VBD'),\n",
       " ('were', 'VBD'),\n",
       " ('be', 'VB'),\n",
       " ('been', 'VBN'),\n",
       " ('being', 'VBG'),\n",
       " ('have', 'VB'),\n",
       " ('has', 'VBZ'),\n",
       " ('had', 'VBN'),\n",
       " ('having', 'VBG'),\n",
       " ('do', 'NN'),\n",
       " ('does', 'VBZ'),\n",
       " ('did', 'VBD'),\n",
       " ('doing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('an', 'DT'),\n",
       " ('the', 'DT'),\n",
       " ('and', 'CC'),\n",
       " ('but', 'CC'),\n",
       " ('if', 'IN'),\n",
       " ('or', 'CC'),\n",
       " ('because', 'IN'),\n",
       " ('as', 'IN'),\n",
       " ('until', 'IN'),\n",
       " ('while', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('at', 'IN'),\n",
       " ('by', 'IN'),\n",
       " ('for', 'IN'),\n",
       " ('with', 'IN'),\n",
       " ('about', 'NN'),\n",
       " ('against', 'IN'),\n",
       " ('between', 'IN'),\n",
       " ('into', 'IN'),\n",
       " ('through', 'IN'),\n",
       " ('during', 'IN'),\n",
       " ('before', 'IN'),\n",
       " ('after', 'IN'),\n",
       " ('above', 'IN'),\n",
       " ('below', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('from', 'IN'),\n",
       " ('up', 'RP'),\n",
       " ('down', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('out', 'RP'),\n",
       " ('on', 'IN'),\n",
       " ('off', 'IN'),\n",
       " ('over', 'IN'),\n",
       " ('under', 'IN'),\n",
       " ('again', 'RB'),\n",
       " ('further', 'JJ'),\n",
       " ('then', 'RB'),\n",
       " ('once', 'RB'),\n",
       " ('here', 'RB'),\n",
       " ('there', 'EX'),\n",
       " ('when', 'WRB'),\n",
       " ('where', 'WRB'),\n",
       " ('why', 'WRB'),\n",
       " ('how', 'WRB'),\n",
       " ('all', 'DT'),\n",
       " ('any', 'DT'),\n",
       " ('both', 'DT'),\n",
       " ('each', 'DT'),\n",
       " ('few', 'JJ'),\n",
       " ('more', 'JJR'),\n",
       " ('most', 'JJS'),\n",
       " ('other', 'JJ'),\n",
       " ('some', 'DT'),\n",
       " ('such', 'JJ'),\n",
       " ('no', 'DT'),\n",
       " ('nor', 'CC'),\n",
       " ('not', 'RB'),\n",
       " ('only', 'RB'),\n",
       " ('own', 'JJ'),\n",
       " ('same', 'JJ'),\n",
       " ('so', 'RB'),\n",
       " ('than', 'IN'),\n",
       " ('too', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('s', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('will', 'MD'),\n",
       " ('just', 'RB'),\n",
       " ('don', 'VB'),\n",
       " (\"don't\", 'NN'),\n",
       " ('should', 'MD'),\n",
       " (\"should've\", 'VB'),\n",
       " ('now', 'RB'),\n",
       " ('d', 'VBZ'),\n",
       " ('ll', 'JJ'),\n",
       " ('m', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('re', 'NN'),\n",
       " ('ve', 'NN'),\n",
       " ('y', 'NN'),\n",
       " ('ain', 'VBP'),\n",
       " ('aren', 'JJ'),\n",
       " (\"aren't\", 'NN'),\n",
       " ('couldn', 'NN'),\n",
       " (\"couldn't\", 'NN'),\n",
       " ('didn', 'VBP'),\n",
       " (\"didn't\", 'NN'),\n",
       " ('doesn', 'NN'),\n",
       " (\"doesn't\", 'NN'),\n",
       " ('hadn', 'VBD'),\n",
       " (\"hadn't\", 'JJ'),\n",
       " ('hasn', 'NN'),\n",
       " (\"hasn't\", 'NN'),\n",
       " ('haven', 'VBD'),\n",
       " (\"haven't\", 'JJ'),\n",
       " ('isn', 'JJ'),\n",
       " (\"isn't\", 'NN'),\n",
       " ('ma', 'VBD'),\n",
       " ('mightn', 'JJ'),\n",
       " (\"mightn't\", 'NN'),\n",
       " ('mustn', 'VBD'),\n",
       " (\"mustn't\", 'JJ'),\n",
       " ('needn', 'JJ'),\n",
       " (\"needn't\", 'NN'),\n",
       " ('shan', 'VBD'),\n",
       " (\"shan't\", 'JJ'),\n",
       " ('shouldn', 'NN'),\n",
       " (\"shouldn't\", 'NN'),\n",
       " ('wasn', 'VBD'),\n",
       " (\"wasn't\", 'JJ'),\n",
       " ('weren', 'NNS'),\n",
       " (\"weren't\", 'VBP'),\n",
       " ('won', 'VBD'),\n",
       " (\"won't\", 'JJ'),\n",
       " ('wouldn', 'NN'),\n",
       " (\"wouldn't\", 'NN')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "nltk.pos_tag(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words='english', strip_accents='unicode', max_df=0.9) #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = MultinomialNB(alpha=0.3, fit_prior=False)\n",
    "# clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43307142857142855\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words='english', strip_accents='unicode') #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = xgb.XGBClassifier()\n",
    "# clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048357142857142855\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words='english', strip_accents='unicode') #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = SVC()\n",
    "# clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40185714285714286\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", strip_accents='unicode', max_df=0.1) #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = RandomForestClassifier(n_estimators = 10)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11571428571428571\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", strip_accents='unicode', max_df=0.1) #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = RandomForestClassifier(max_depth=5, n_estimators = 10)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16085714285714287\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", strip_accents='unicode', max_df=0.1) #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = RandomForestClassifier(max_depth=10, n_estimators = 10)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", strip_accents='unicode', max_df=0.1) #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = RandomForestClassifier(max_depth=4000, n_estimators = 1000)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", strip_accents='unicode', max_df=0.1) #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = RandomForestClassifier(max_depth=1000, n_estimators = 4000)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", strip_accents='unicode', max_df=0.1) #, max_df=0.95 )\n",
    "GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "GX_test_idf = tfidf.transform(Gx_test)\n",
    "clf = RandomForestClassifier(max_depth=9000, n_estimators = 9000)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5729285714285715\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\", smooth_idf=True, sublinear_tf=True, use_idf=True, strip_accents='unicode',min_df=1) #, max_df=0.95 )\n",
    "multiNB = MultinomialNB(alpha=0.3)\n",
    "\n",
    "def getScoretWithModel(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    return model.score(x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "# for train_index, test_index in kf.split(commentsTrain, subredditsTrain):\n",
    "x_train, x_test, y_train, y_test = train_test_split(commentsTrain, subredditsTrain, test_size=0.2, random_state=4)\n",
    "redditDataTrainTF = tfidf.fit_transform(x_train)\n",
    "redditDataTestTF = tfidf.transform(x_test)\n",
    "    # redditDataTrainTF.toarray()\n",
    "    # print(redditDataTrainTF.shape)\n",
    "    # feature_scores = mutual_info_classif(redditDataTrainTF, y_train1)\n",
    "    # print(feature_scores)\n",
    "print(getScoretWithModel(multiNB, redditDataTrainTF, redditDataTestTF, y_train, y_test))\n",
    "#     print(results)\n",
    "# print(sum(results)/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words='english', strip_accents='unicode', max_df=0.9) #, max_df=0.95 )\n",
    "# Gx_train, Gx_test, y_train, y_test = train_test_split(commentsTrain, subredditsTrain, test_size=0.2, random_state=4)\n",
    "# GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "# GX_test_idf = tfidf.transform(Gx_test)\n",
    "# final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "# print(final_acc)\n",
    "\n",
    "# tfidf = TfidfVectorizer(stop_words='english', strip_accents='unicode', max_df=0.1) #, max_df=0.95 )\n",
    "# GX_train_idf = tfidf.fit_transform(Gx_train)\n",
    "# GX_test_idf = tfidf.transform(Gx_test)\n",
    "# clf = MultinomialNB(alpha=0.3, fit_prior=False)\n",
    "# # clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "# final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, y_train, y_test)\n",
    "# print(final_acc)\n",
    "\n",
    "df = pd.read_csv(\"data/reddit_test.csv\", )\n",
    "actual_Xtest = df.comments\n",
    "\n",
    "commentsTrainidf = tfidf.fit_transform(commentsTrain)\n",
    "actual_Xtestidf = tfidf.transform(actual_Xtest)\n",
    "\n",
    "clf = MultinomialNB(alpha=0.3, fit_prior=False)\n",
    "clf.fit(commentsTrainidf, subredditsTrain)\n",
    "\n",
    "pred = clf.predict(actual_Xtestidf)\n",
    "# print(pred)\n",
    "pred = pd.DataFrame(pred, columns=['Category']).to_csv(\"results/testResults6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/reddit_test.csv\", )\n",
    "actual_Xtest = df.comments\n",
    "\n",
    "commentsTrainidf = tfidf.fit_transform(commentsTrain)\n",
    "actual_Xtestidf = tfidf.transform(actual_Xtest)\n",
    "\n",
    "clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "clf.fit(commentsTrainidf, subredditsTrain)\n",
    "\n",
    "pred = clf.predict(actual_Xtestidf)\n",
    "# print(pred)\n",
    "pred = pd.DataFrame(pred, columns=['Category']).to_csv(\"results/testResults3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/test1.csv\", )\n",
    "df = df[['comments','subreddits']]\n",
    "df.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-9abf7d86effc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothing_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfinal_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetScoretWithModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGX_train_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_Xtest_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_ytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4a0d1c5ac1b9>\u001b[0m in \u001b[0;36mgetScoretWithModel\u001b[0;34m(model, x_train, x_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetScoretWithModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/courses/comp551/mini_project1/mini_project_2/models/naive_bayes.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/courses/comp551/mini_project1/mini_project_2/models/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthetajk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthetajk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtemp_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "total_Xtrain = commentsTrain\n",
    "total_Xtrain_idf = tfidf.fit_transform(total_Xtrain)\n",
    "\n",
    "total_ytrain = subredditsTrain\n",
    "\n",
    "actual_Xtest = df.comments\n",
    "actual_Xtest_idf = tfidf.transform(actual_Xtest)\n",
    "\n",
    "actual_ytest = df.subreddits\n",
    "\n",
    "\n",
    "clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, actual_Xtest_idf, Gy_train, actual_ytest)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "0.5657857142857143\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words='english', strip_accents='ascii', ngram_range=(1, 1), lowercase=True, tokenizer = token.tokenize)\n",
    "GX_train_idf = tfidf.fit_transform(GX_train)\n",
    "GX_test_idf = tfidf.transform(GX_test)\n",
    "clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, Gy_train, Gy_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5619285714285714\n"
     ]
    }
   ],
   "source": [
    "def stem_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words, strip_accents='ascii', ngram_range=(1, 1), lowercase=True, tokenizer = stem_tokenize)\n",
    "GX_train_idf = tfidf.fit_transform(GX_train)\n",
    "GX_test_idf = tfidf.transform(GX_test)\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, Gy_train, Gy_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing factor 0.1\n",
      "0.5658571428571428\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "GX_train_idf = tfidf.fit_transform(GX_train)\n",
    "GX_test_idf = tfidf.transform(GX_test)\n",
    "clf = CustomNaiveBayes(smoothing_factor=0.1)\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, Gy_train, Gy_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32971428571428574\n"
     ]
    }
   ],
   "source": [
    "GX_train_idf = tfidf.fit_transform(GX_train)\n",
    "GX_test_idf = tfidf.transform(GX_test)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "final_acc = getScoretWithModel(clf, GX_train_idf, GX_test_idf, Gy_train, Gy_test)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/test1.csv\", )\n",
    "df = df[['comments','subreddits']]\n",
    "df.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_testX = df.comments\n",
    "actual_testy = df.subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
