{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numbers\n",
    "import decimal\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import stdev\n",
    "from statistics import mean\n",
    "import time\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditDataTrain = pd.read_csv(\"data/reddit_train.csv\") #, sep=\"\\n\", header=None) \n",
    "redditDataTest = pd.read_csv(\"data/reddit_test.csv\") # sep=\"\\n\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Honestly, Buffalo is the correct answer. I rem...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ah yes way could have been :( remember when he...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>He wouldn't have been a bad signing if we woul...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Easy. You use the piss and dry technique. Let ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69995</td>\n",
       "      <td>69995</td>\n",
       "      <td>Thank you, you confirm Spain does have nice pe...</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69996</td>\n",
       "      <td>69996</td>\n",
       "      <td>Imagine how many he would have killed with a r...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69997</td>\n",
       "      <td>69997</td>\n",
       "      <td>Yes. Only. As in the guy I was replying to was...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69998</td>\n",
       "      <td>69998</td>\n",
       "      <td>Looking for something light-hearted or has a v...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69999</td>\n",
       "      <td>69999</td>\n",
       "      <td>I love how I never cry about casters because I...</td>\n",
       "      <td>GlobalOffensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           comments  \\\n",
       "0          0  Honestly, Buffalo is the correct answer. I rem...   \n",
       "1          1  Ah yes way could have been :( remember when he...   \n",
       "2          2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...   \n",
       "3          3  He wouldn't have been a bad signing if we woul...   \n",
       "4          4  Easy. You use the piss and dry technique. Let ...   \n",
       "...      ...                                                ...   \n",
       "69995  69995  Thank you, you confirm Spain does have nice pe...   \n",
       "69996  69996  Imagine how many he would have killed with a r...   \n",
       "69997  69997  Yes. Only. As in the guy I was replying to was...   \n",
       "69998  69998  Looking for something light-hearted or has a v...   \n",
       "69999  69999  I love how I never cry about casters because I...   \n",
       "\n",
       "            subreddits  \n",
       "0               hockey  \n",
       "1                  nba  \n",
       "2      leagueoflegends  \n",
       "3               soccer  \n",
       "4                funny  \n",
       "...                ...  \n",
       "69995           europe  \n",
       "69996  leagueoflegends  \n",
       "69997           canada  \n",
       "69998            anime  \n",
       "69999  GlobalOffensive  \n",
       "\n",
       "[70000 rows x 3 columns]"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditDataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allClasses = [\"hockey\", \"nba\", \"leagueoflegends\", \"soccer\", \"funny\", \"movies\", \"anime\", \"Overwatch\", \"trees\", \"GlobalOffensive\", \"nfl\", \"AskReddit\", \"gameofthrones\", \"conspiracy\", \"worldnews\", \"wow\", \"europe\", \"canada\", \"Music\", \"baseball\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(commentsTrain, subredditsTrain, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31604    Irrational silly people who call themselves St...\n",
       "10793    Last time I checked none of them have champion...\n",
       "52093    feels rlly bad brushing after drinking coke yo...\n",
       "25134    If anything it has very little to do with the ...\n",
       "31569    It's about 30-40 km east of Winnipeg. There is...\n",
       "                               ...                        \n",
       "46393    1 minute build up with a single four second ca...\n",
       "20062    Because that's the corner they've painted them...\n",
       "65276          I wish but I can't watch what he's from ;n;\n",
       "23346    He saved you the trouble of typing out somethi...\n",
       "11863    First time viewer here. I feel a bit sorry for...\n",
       "Name: comments, Length: 56000, dtype: object"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentsTrain = redditDataTrain.iloc[:,1]\n",
    "subredditsTrain = redditDataTrain.iloc[:,-1]\n",
    "commentsTest = redditDataTest.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hockey', 'nba', 'leagueoflegends', 'soccer', 'funny', 'movies',\n",
       "       'anime', 'Overwatch', 'trees', 'GlobalOffensive', 'nfl',\n",
       "       'AskReddit', 'gameofthrones', 'conspiracy', 'worldnews', 'wow',\n",
       "       'europe', 'canada', 'Music', 'baseball'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subredditsTrain.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "cv = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "multiNB = MultinomialNB()\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "kf = KFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditDataTrainTF = tfidf.fit_transform(x_train)\n",
    "redditDataTestTF = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoretWithModel(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    return model.score(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GlobalOffensive', 'GlobalOffensive', 'GlobalOffensive', 'hockey'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiNB.fit(redditDataTrainTF[:4], y_train[:4])\n",
    "multiNB.predict(redditDataTestTF[:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5506428571428571\n"
     ]
    }
   ],
   "source": [
    "print(getScoretWithModel(lr, redditDataTrainTF, redditDataTestTF, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5637142857142857\n"
     ]
    }
   ],
   "source": [
    "print(getScoretWithModel(multiNB, redditDataTrainTF, redditDataTestTF, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nfl', 'hockey', 'GlobalOffensive'], dtype=object)"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True, stop_words='english')\n",
    "X= x_train[:3]\n",
    "# X.append(pd.Series(x_train[2]))\n",
    "X = cv.fit_transform(x_train[:3])\n",
    "y = y_train[:3]\n",
    "# y.append(y_train[2])\n",
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "A1 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-932-c259788da839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilteredX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfilteredX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: A1 not found"
     ]
    }
   ],
   "source": [
    "indices = np.where(y==k)[0]\n",
    "filteredX = X[[0,2]]\n",
    "filteredX.A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[<25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>,\n",
       "         <25x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Column format>]],\n",
       "       dtype=object)"
      ]
     },
     "execution_count": 969,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k='nfl'\n",
    "thetajk[k] = (filteredX.sum(axis=0))/filteredX.shape[0]\n",
    "# thetajk[k]\n",
    "# float(filteredX.shape[0])+2\n",
    "y = thetajk[k].dot(x.T)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "thetak = defaultdict(float)\n",
    "thetajk = defaultdict(partial(np.ndarray, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredX.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {'nfl': 0.3333333333333333, 'hockey': 0.3333333333333333, 'GlobalOffensive': 0.3333333333333333}) [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.66666667 0.33333333 0.66666667 0.33333333\n",
      " 0.33333333 0.33333333 0.66666667 0.33333333 0.66666667 0.33333333\n",
      " 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.33333333\n",
      " 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "for k in y.values:\n",
    "    thetak[k] = (y==k).sum()/float(y.shape[0])\n",
    "    indices = np.where(y==k)[0]\n",
    "    filteredX = X[indices]\n",
    "    thetajk[k] = (filteredX.toarray().sum(axis=0)+1)/(float(filteredX.shape[0]+2))\n",
    "print(thetak, thetajk['nfl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-16.08727025529183, -18.859858977531612, -17.47356461641172]"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=[\"nfl\", \"hockey\", \"GlobalOffensive\"]\n",
    "class_prob=[]\n",
    "for k in classes:\n",
    "    likelihood=0\n",
    "    for j in range(len(Xtest)):\n",
    "        likelihood += Xtest[j]*np.log(thetajk[k][j]) + (1-Xtest[j])*(np.log(1-thetajk[k][j]))\n",
    "    class_prob.append(likelihood + np.log(thetak[k]))\n",
    "class_prob\n",
    "# Xtest[j]*np.log(thetajk[k][j]) + (1-Xtest[j])*(np.log(1-thetajk[k][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0986122986681097"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.33333333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest = redditDataTestTF[:4].toarray()[0]\n",
    "Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.66666667, 0.33333333, 0.66666667, 0.33333333,\n",
       "       0.33333333, 0.66666667, 0.66666667, 0.33333333, 0.66666667,\n",
       "       0.33333333, 0.33333333, 0.66666667, 0.33333333, 0.33333333,\n",
       "       0.66666667, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
       "       0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetajk[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65889"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 25 is out of bounds for axis 0 with size 25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-706-45b700738479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mthetajk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 25 is out of bounds for axis 0 with size 25"
     ]
    }
   ],
   "source": [
    "test=0\n",
    "for j in range(len(Xtest)):\n",
    "    test += (1-Xtest[j])*(np.log(1-thetajk[k][j]))\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "class MultiNomialNB:\n",
    "    def __init__(self):\n",
    "        self.thetak = defaultdict(float)\n",
    "        self.thetajk = defaultdict(partial(np.ndarray, 0))\n",
    "        self.classes = y.values\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for k in y.values:\n",
    "            self.thetak[k] = (y==k).sum()/float(y.shape[0])\n",
    "            indices = np.where(y==k)[0]\n",
    "            filteredX = X[indices]\n",
    "            self.thetajk[k] = (filteredX.toarray().sum(axis=0)+1)/(float(filteredX.shape[0]+2))\n",
    "        print(self.thetak, len(self.thetajk[k]))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y=[]\n",
    "        count=0\n",
    "        for x in X.toarray():\n",
    "            max_class_probab = float('-inf')\n",
    "            most_probab_class=''\n",
    "            for k in self.classes:\n",
    "                likelihood = 0.0\n",
    "                #TODO: conver this to a vector summation\n",
    "                liklihood_v = x*np.log(self.thetajk[k]) + (1-x)*(np.log(1-self.thetajk[k]))\n",
    "                curr_class_probab = liklihood_v.sum() + np.log(self.thetak[k])\n",
    "                if curr_class_probab > max_class_probab:\n",
    "                    max_class_probab = curr_class_probab\n",
    "                    most_probab_class = k\n",
    "                    \n",
    "            y.append(most_probab_class)\n",
    "        \n",
    "        return np.array(y)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        score = (y_pred==y).sum()/float(y.shape[0])\n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmultiNB = MultiNomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True, stop_words='english')\n",
    "x_train_v = cv.fit_transform(x_train[:3])\n",
    "x_test_v = cv.transform(x_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {'nfl': 0.3333333333333333, 'hockey': 0.3333333333333333, 'GlobalOffensive': 0.3333333333333333}) 25\n"
     ]
    }
   ],
   "source": [
    "cmultiNB.fit(x_train_v, y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmultiNB.score(x_test_v, pd.Series(['GlobalOffensive', 'l', 'l']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseball'"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_v[::-1]\n",
    "y_test.unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.matrix([0]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-978-111d57c0ffc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "a[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "redditDataTrain = pd.read_csv(\"data/reddit_train.csv\") #, sep=\"\\n\", header=None) \n",
    "redditDataTest = pd.read_csv(\"data/reddit_test.csv\") # sep=\"\\n\", header=None)\n",
    "\n",
    "redditDataTrain = redditDataTrain\n",
    "\n",
    "commentsTrain = redditDataTrain.iloc[:,1]\n",
    "subredditsTrain = redditDataTrain.iloc[:,-1]\n",
    "commentsTest = redditDataTest.iloc[:,1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(commentsTrain, subredditsTrain, test_size=0.2, random_state=4)\n",
    "\n",
    "cv = CountVectorizer(binary=True, stop_words='english')\n",
    "x_train_v = cv.fit_transform(commentsTrain[:4].append(commentsTrain[:4], ignore_index=True))\n",
    "x_test_v = cv.transform(commentsTrain[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             hockey\n",
      "1                nba\n",
      "2    leagueoflegends\n",
      "3             soccer\n",
      "4         conspiracy\n",
      "5                wow\n",
      "6              funny\n",
      "7          AskReddit\n",
      "Name: subreddits, dtype: object\n",
      "i [0]\n",
      "f [[1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0\n",
      "  1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0]]\n",
      "[0.99019608 0.00980392 0.00980392 0.00980392 0.00980392 0.00980392\n",
      " 0.99019608 0.00980392 0.99019608 0.99019608 0.00980392 0.99019608\n",
      " 0.99019608 0.00980392 0.00980392 0.99019608 0.99019608 0.00980392\n",
      " 0.99019608 0.00980392 0.99019608 0.00980392 0.00980392 0.99019608\n",
      " 0.99019608 0.00980392 0.00980392 0.99019608 0.00980392 0.00980392\n",
      " 0.99019608 0.00980392 0.00980392 0.99019608 0.99019608 0.00980392\n",
      " 0.99019608 0.00980392 0.00980392 0.99019608 0.00980392 0.99019608\n",
      " 0.99019608 0.00980392 0.00980392 0.99019608 0.99019608 0.99019608\n",
      " 0.99019608 0.99019608 0.00980392 0.00980392 0.99019608 0.99019608\n",
      " 0.00980392 0.99019608 0.00980392 0.99019608 0.00980392 0.00980392]\n",
      "i [1]\n",
      "f [[0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0]]\n",
      "[0.00980392 0.00980392 0.00980392 0.00980392 0.00980392 0.99019608\n",
      " 0.00980392 0.00980392 0.00980392 0.00980392 0.99019608 0.00980392\n",
      " 0.00980392 0.00980392 0.00980392 0.00980392 0.00980392 0.00980392\n",
      " 0.00980392 0.99019608 0.00980392 0.00980392 0.00980392 0.00980392\n",
      " 0.00980392 0.99019608 0.99019608 0.00980392 0.00980392 0.99019608\n",
      " 0.00980392 0.00980392 0.99019608 0.00980392 0.00980392 0.99019608\n",
      " 0.00980392 0.00980392 0.00980392 0.00980392 0.00980392 0.99019608\n",
      " 0.00980392 0.00980392 0.00980392 0.00980392 0.00980392 0.00980392\n",
      " 0.00980392 0.00980392 0.99019608 0.99019608 0.00980392 0.00980392\n",
      " 0.99019608 0.00980392 0.00980392 0.00980392 0.99019608 0.00980392]\n",
      "defaultdict(<class 'float'>, {'hockey': 0.125, 'nba': 0.125}) (8, 60)\n"
     ]
    }
   ],
   "source": [
    "X = x_train_v\n",
    "y = subredditsTrain[:4].append(subredditsTrain[99:103], ignore_index=True)\n",
    "print(y)\n",
    "thetak=defaultdict(float)\n",
    "classes = y.unique()\n",
    "thetajk = np.arange(len(classes)*X.shape[1], dtype=float).reshape(len(classes),X.shape[1])\n",
    "\n",
    "for i,k in enumerate(classes[:2]):\n",
    "    thetak[k] = (y==k).sum()/float(y.shape[0])\n",
    "    indices = np.where(y==k)[0]\n",
    "    print('i', indices)\n",
    "    filteredX = X[indices]\n",
    "    print(\"f\", filteredX.toarray())\n",
    "    thetajk[i] = (filteredX.sum(axis=0)+0.01)/float(filteredX.shape[0]+0.02)\n",
    "    print(thetajk[i])\n",
    "print(thetak, thetajk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 60) (8, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in log\n",
      "  \"\"\"\n",
      "/Users/faizank/Documents/courses/comp551/mini_project1/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 1310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x_test_v\n",
    "print(X.shape, thetajk.shape)\n",
    "# right = (1-X.toarray()).dot(np.subtract(1, np.log(thetajk).T))\n",
    "left = X.dot(np.log(thetajk).T)\n",
    "middle = np.array([1]*X.shape[0]*X.shape[1]).reshape(X.shape).dot(np.log(1-thetajk).T)\n",
    "right = X.dot(np.log(1-thetajk).T)\n",
    "temp = np.add(left, middle, right)\n",
    "temp.shape\n",
    "# left.shape, middle.shape, right.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 1242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array([1]*X.shape[0]*X.shape[1]).reshape(X.shape[0],X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
