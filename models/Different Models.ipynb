{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1000)\n",
      "[11 14 12 ...  6  4  1]\n",
      "ready\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7cbd28bb2a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#     classifier.score(x_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class MultiNomialNB:\n",
    "    def __init__(self):\n",
    "        self.thetak = defaultdict(float)\n",
    "        # self.thetajk = defaultdict(partial(np.ndarray, 0))\n",
    "        self.thetajk= []\n",
    "        self.classes = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes = y.unique()\n",
    "        self.thetajk = np.arange(len(self.classes)*X.shape[1], dtype=float).reshape(len(self.classes),X.shape[1])\n",
    "\n",
    "        for i,k in enumerate(self.classes):\n",
    "            self.thetak[k] = (y==k).sum()/float(y.shape[0])\n",
    "            indices = np.where(y==k)[0]\n",
    "            filteredX = X[indices]\n",
    "            self.thetajk[i] = (filteredX.sum(axis=0)+0.01)/float(filteredX.shape[0]+0.02)\n",
    "            self.thetajk[i] = self.thetajk[i]\n",
    "        print(self.thetak, self.thetajk)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        left = X.dot(np.log(self.thetajk).T)\n",
    "        right = (1-X.toarray()).dot(np.log(1-self.thetajk).T)\n",
    "        temp_sum = np.add(left, right)\n",
    "        max_num = np.argmax(temp_sum,axis=1)\n",
    "        return self.classes[max_num]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        score = (y_pred==y).sum()/float(y.shape[0])\n",
    "        return score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    redditData = pd.read_csv(\"../data/reddit_train.csv\") #, sep=\"\\n\", header=None) \n",
    "    redditDataTest = pd.read_csv(\"../data/reddit_test.csv\") # sep=\"\\n\", header=None)\n",
    "\n",
    "    comments = redditData.iloc[:,1]\n",
    "    subreddits = redditData.iloc[:,-1]\n",
    "    comments = [str (item) for item in comments]\n",
    "    \n",
    "    tf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    comments = tf.fit_transform(comments).toarray()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    subreddits = le.fit_transform(subreddits)\n",
    "    print(comments.shape)\n",
    "    print(subreddits)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(comments, subreddits, test_size=0.2, random_state=4)\n",
    "    \n",
    "    print(\"ready\")\n",
    "#     classifier = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "#     classifier.fit(x_train, y_train)\n",
    "#     classifier.score(x_test, y_test)\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(1000, 100, input_length=x_train.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(13, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    epochs = 5\n",
    "    batch_size = 64\n",
    "\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    \n",
    "    accr = model.evaluate(X_test,Y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    cmultiNB = MultiNomialNB()\n",
    "    cmultiNB.fit(x_train_v, y_train)\n",
    "\n",
    "#     print(cmultiNB.score(x_test_v, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2681428571428571\n"
     ]
    }
   ],
   "source": [
    "print(classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MAIS-202)",
   "language": "python",
   "name": ".mais-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
